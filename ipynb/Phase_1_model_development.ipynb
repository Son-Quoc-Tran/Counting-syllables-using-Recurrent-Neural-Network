{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase 1 model development.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orkRMxJoW1J",
        "colab_type": "text"
      },
      "source": [
        "#***Experimenting LSTM for counting syllables in words***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeuvQGRCoGbL",
        "colab_type": "text"
      },
      "source": [
        "This notebook is used for developing model in the phase 1. The model used is unidirectional LSTM.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RUzb6OxdeA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import all used library\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np \n",
        "from collections import Counter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as pyplot\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLXW-ospdnSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "1f12b810-4d92-47ef-d992-6ad9bf694982"
      },
      "source": [
        "#Load data using pandas\n",
        "path_to_dictionary = '/content/converted_syllable_dict.csv'\n",
        "print('Parsing the dataset .csv file...')\n",
        "dictionary = pd.read_csv(path_to_dictionary)\n",
        "dictionary = shuffle(dictionary)\n",
        "print(dictionary)\n",
        "print('Finished')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing the dataset .csv file...\n",
            "                 word  syllable_count\n",
            "32098       downsides               2\n",
            "84812           penis               2\n",
            "92183   realistically               5\n",
            "26376          dahl's               1\n",
            "9315         beilfuss               2\n",
            "...               ...             ...\n",
            "20468       civilized               3\n",
            "119332       vertebra               3\n",
            "63650          lashes               2\n",
            "11313          blaine               1\n",
            "74349    misinforming               4\n",
            "\n",
            "[125914 rows x 2 columns]\n",
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn8t6zEddpPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = dictionary['word']\n",
        "myWords = []\n",
        "for word in words:\n",
        "    myWords.append(str(word).lower())\n",
        "myCounts = dictionary['syllable_count']\n",
        "count_list =[]\n",
        "for count in myCounts:\n",
        "    count_list.append(count * 1000)\n",
        "counts = np.array(count_list) #convert count_list into np.array"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5i3j5FLdrKZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7acc8763-b42d-46e1-de0b-af3df2f05409"
      },
      "source": [
        "all_words = ' '.join(str(myWords))\n",
        "chars = all_words.split()\n",
        "counters = Counter(chars)\n",
        "char_list = sorted(counters, key=counters.get, reverse=True)\n",
        "char_to_int = {char: ii for ii, char in enumerate(char_list, 1)}\n",
        "print(char_to_int)\n",
        "\n",
        "words_ints =[]\n",
        "for word in myWords:\n",
        "    words_ints.append([char_to_int[char] for char in str(word)])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"'\": 1, ',': 2, 'e': 3, 'a': 4, 'r': 5, 'i': 6, 's': 7, 'n': 8, 'o': 9, 't': 10, 'l': 11, 'c': 12, 'd': 13, 'm': 14, 'u': 15, 'h': 16, 'g': 17, 'p': 18, 'b': 19, 'k': 20, 'y': 21, '\"': 22, 'f': 23, 'w': 24, 'v': 25, 'z': 26, 'j': 27, 'x': 28, 'q': 29, '-': 30, '.': 31, '[': 32, '1': 33, ']': 34}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIp8SW5DdtMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f92f2c5-77fd-406e-eb0f-d4f600c0881a"
      },
      "source": [
        "len_word = [len(word) for word in myWords]\n",
        "max_length = max(len_word)\n",
        "print(max_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHSko36eIq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_words(words, length):\n",
        "  features = np.zeros((len(words), length), dtype=int)\n",
        "  for i, row in enumerate(words):\n",
        "    features[i, -len(row):] = np.array(row)[:length]\n",
        "  return features"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jer_KS40e1Vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "cf572169-48ee-4593-c518-70c8ae8885f9"
      },
      "source": [
        "myLength = max_length\n",
        "features = pad_words(words_ints, myLength)\n",
        "print(features[-30:, -10:])\n",
        "print(len(features))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0 18  5  9 10  5 15 13  3]\n",
            " [ 0  0  0  0 19  6  3 19  3  5]\n",
            " [ 0  0  0 17  5  6 23 23  3  3]\n",
            " [ 0  0 20  6 20 20  9 14  4  8]\n",
            " [ 0  0  7 12 16  5  3  6  3  5]\n",
            " [ 0  0  8  3 24 25  6 11 11  3]\n",
            " [ 4 10  4 18  9  6  8 10  1  7]\n",
            " [ 5 20  6  8  7  9  8  6  7 14]\n",
            " [ 0  0  0  0  0  0  8 15 13  9]\n",
            " [ 0  0  0  0  0  0 17  9 10  9]\n",
            " [ 0  0  0  0  5  3 13  3 11 11]\n",
            " [ 0  0  7  6 11  3  8 12  3  7]\n",
            " [ 0  0  0 16 15  5  5  6  3  7]\n",
            " [ 3 10 16  3  5 11  4  8 13  7]\n",
            " [ 0  0  0  0  0  0 17  6  5 10]\n",
            " [ 0  0  0  0  0 16  4 11 11  3]\n",
            " [ 0  0  0 27  4 12  9 19 15  7]\n",
            " [10 24  4  5 13  9 24  7 20  6]\n",
            " [ 0  0  0  0  0  0 19  9 14 19]\n",
            " [ 0  0  7 10 30 13  3  8  6  7]\n",
            " [ 0 18  4  5 10 16  3  8  9  8]\n",
            " [ 0  0 11  3  6  7 16 14  4  8]\n",
            " [ 0 14  4 12 20  9 24  7 20  6]\n",
            " [ 0  0 12 15 11 11 15 14  1  7]\n",
            " [19  7  3  5 25  4 10  6  9  8]\n",
            " [ 0 12  6 25  6 11  6 26  3 13]\n",
            " [ 0  0 25  3  5 10  3 19  5  4]\n",
            " [ 0  0  0  0 11  4  7 16  3  7]\n",
            " [ 0  0  0  0 19 11  4  6  8  3]\n",
            " [ 7  6  8 23  9  5 14  6  8 17]]\n",
            "125914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX8UMAE6fHhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "082bb0d3-5fe1-4b73-9994-1722e6ee6863"
      },
      "source": [
        "split_frac = 0.9\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*split_frac)\n",
        "print(split_idx)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = counts[:split_idx], counts[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113322\n",
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(113322, 28) \n",
            "Validation set: \t(6296, 28) \n",
            "Test set: \t\t(6296, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyVh8dXxfiXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO4odJfPha7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "24ff774d-afbd-485b-fd74-bd1289642ce7"
      },
      "source": [
        "import torch\n",
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "texatJRvhexG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
        "        #dropout layers\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # linear layers\n",
        "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, output_size)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.dropout(lstm_out)\n",
        "        # fully-connected layer\n",
        "        out = nn.functional.relu(self.fc1(lstm_out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:, -1] # get last batch of labels\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQr33JSFh1p2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "0dc3ae37-26ab-4303-90ba-cd40c99e71e8"
      },
      "source": [
        "vocab_size = len(char_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net_lstm = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print('LSTM \\n', net_lstm )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM \n",
            " LSTM(\n",
            "  (embedding): Embedding(35, 100)\n",
            "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqt0A-Sdi3sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.0005\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net_lstm.parameters(), lr=lr)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJyyzRu_ji_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#functions used for training models\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def save_model(filename, decoder):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "\n",
        "def load_model(filename):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    return torch.load(save_filename)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXSctqyVjkiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0164114-4201-40c7-f731-14ba9a9de7b0"
      },
      "source": [
        "epochs = 10 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "#lists used for plotting losses\n",
        "step_list = list()\n",
        "train_loss_list = list()\n",
        "val_loss_list = list()\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net_lstm.cuda()\n",
        "\n",
        "#use min_loss to track the best model throughout the training loop\n",
        "min_val_loss = math.inf\n",
        "\n",
        "net_lstm.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # initialize hidden state\n",
        "    h = net_lstm.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net_lstm.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, _ = net_lstm(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net_lstm.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Get validation loss\n",
        "            val_h = net_lstm.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net_lstm.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net_lstm(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "\n",
        "            net_lstm.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Time: {:}\".format(elapsed))\n",
        "            if np.mean(val_losses) < min_val_loss:\n",
        "                min_val_loss = np.mean(val_losses)\n",
        "                save_model('./save/trained_lstm', net_lstm)\n",
        "                print('Model Trained and Saved')\n",
        "            step_list.append(counter)\n",
        "            train_loss_list.append(loss.item())\n",
        "            val_loss_list.append(np.mean(val_losses))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "Epoch: 1/10... Step: 100... Loss: 7037529.000000... Val Loss: 6410347.000000 Time: 0:00:02\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 200... Loss: 5112251.000000... Val Loss: 5480363.340000 Time: 0:00:04\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 300... Loss: 4121836.000000... Val Loss: 4228218.100000 Time: 0:00:06\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 400... Loss: 3486879.250000... Val Loss: 2855048.710000 Time: 0:00:09\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 500... Loss: 1555256.250000... Val Loss: 1647837.214000 Time: 0:00:11\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 600... Loss: 1612607.125000... Val Loss: 985261.447250 Time: 0:00:13\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 700... Loss: 924717.000000... Val Loss: 958156.428750 Time: 0:00:16\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 800... Loss: 911599.312500... Val Loss: 958815.454250 Time: 0:00:18\n",
            "Epoch: 1/10... Step: 900... Loss: 717890.625000... Val Loss: 703011.587750 Time: 0:00:20\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1000... Loss: 294790.968750... Val Loss: 380371.716000 Time: 0:00:22\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1100... Loss: 290361.968750... Val Loss: 252578.980812 Time: 0:00:25\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1200... Loss: 225189.796875... Val Loss: 166614.488281 Time: 0:00:27\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1300... Loss: 314269.781250... Val Loss: 146397.849469 Time: 0:00:29\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1400... Loss: 252725.796875... Val Loss: 125403.382094 Time: 0:00:32\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1500... Loss: 170071.609375... Val Loss: 120768.032812 Time: 0:00:34\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1600... Loss: 89882.320312... Val Loss: 108065.372078 Time: 0:00:36\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1700... Loss: 45687.417969... Val Loss: 100623.402156 Time: 0:00:38\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1800... Loss: 83712.289062... Val Loss: 95483.837719 Time: 0:00:41\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 1900... Loss: 112187.375000... Val Loss: 120569.587594 Time: 0:00:43\n",
            "Epoch: 1/10... Step: 2000... Loss: 109832.976562... Val Loss: 97075.173625 Time: 0:00:45\n",
            "Epoch: 1/10... Step: 2100... Loss: 146879.531250... Val Loss: 91317.168859 Time: 0:00:47\n",
            "Model Trained and Saved\n",
            "Epoch: 1/10... Step: 2200... Loss: 34659.257812... Val Loss: 87663.226344 Time: 0:00:50\n",
            "Model Trained and Saved\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "Epoch: 2/10... Step: 2300... Loss: 134852.812500... Val Loss: 85381.951703 Time: 0:00:01\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 2400... Loss: 56557.253906... Val Loss: 89530.871937 Time: 0:00:03\n",
            "Epoch: 2/10... Step: 2500... Loss: 115471.539062... Val Loss: 88848.327516 Time: 0:00:05\n",
            "Epoch: 2/10... Step: 2600... Loss: 40943.242188... Val Loss: 78235.148734 Time: 0:00:07\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 2700... Loss: 91778.398438... Val Loss: 78762.556750 Time: 0:00:10\n",
            "Epoch: 2/10... Step: 2800... Loss: 125650.937500... Val Loss: 78333.683008 Time: 0:00:12\n",
            "Epoch: 2/10... Step: 2900... Loss: 66505.882812... Val Loss: 79411.420250 Time: 0:00:14\n",
            "Epoch: 2/10... Step: 3000... Loss: 110112.578125... Val Loss: 73266.945156 Time: 0:00:17\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 3100... Loss: 74378.656250... Val Loss: 68343.754281 Time: 0:00:19\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 3200... Loss: 82747.218750... Val Loss: 79226.391852 Time: 0:00:21\n",
            "Epoch: 2/10... Step: 3300... Loss: 135212.203125... Val Loss: 82558.139359 Time: 0:00:23\n",
            "Epoch: 2/10... Step: 3400... Loss: 48540.597656... Val Loss: 82994.188812 Time: 0:00:26\n",
            "Epoch: 2/10... Step: 3500... Loss: 40203.296875... Val Loss: 66021.039102 Time: 0:00:28\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 3600... Loss: 34432.707031... Val Loss: 68100.143977 Time: 0:00:30\n",
            "Epoch: 2/10... Step: 3700... Loss: 135563.640625... Val Loss: 65898.515383 Time: 0:00:32\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 3800... Loss: 96023.507812... Val Loss: 66869.275844 Time: 0:00:35\n",
            "Epoch: 2/10... Step: 3900... Loss: 40602.847656... Val Loss: 66558.731609 Time: 0:00:37\n",
            "Epoch: 2/10... Step: 4000... Loss: 45704.589844... Val Loss: 65634.053453 Time: 0:00:39\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 4100... Loss: 94823.820312... Val Loss: 70865.685016 Time: 0:00:42\n",
            "Epoch: 2/10... Step: 4200... Loss: 79921.664062... Val Loss: 62194.157813 Time: 0:00:44\n",
            "Model Trained and Saved\n",
            "Epoch: 2/10... Step: 4300... Loss: 56545.644531... Val Loss: 71569.426250 Time: 0:00:46\n",
            "Epoch: 2/10... Step: 4400... Loss: 36516.031250... Val Loss: 70693.554359 Time: 0:00:48\n",
            "Epoch: 2/10... Step: 4500... Loss: 66583.195312... Val Loss: 64750.549305 Time: 0:00:51\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "Epoch: 3/10... Step: 4600... Loss: 67898.835938... Val Loss: 59097.768523 Time: 0:00:01\n",
            "Model Trained and Saved\n",
            "Epoch: 3/10... Step: 4700... Loss: 65848.187500... Val Loss: 62546.546348 Time: 0:00:03\n",
            "Epoch: 3/10... Step: 4800... Loss: 43916.304688... Val Loss: 65041.897430 Time: 0:00:06\n",
            "Epoch: 3/10... Step: 4900... Loss: 57387.945312... Val Loss: 57771.973547 Time: 0:00:08\n",
            "Model Trained and Saved\n",
            "Epoch: 3/10... Step: 5000... Loss: 110761.593750... Val Loss: 62109.760313 Time: 0:00:10\n",
            "Epoch: 3/10... Step: 5100... Loss: 46955.449219... Val Loss: 60606.801547 Time: 0:00:13\n",
            "Epoch: 3/10... Step: 5200... Loss: 49227.347656... Val Loss: 67922.727516 Time: 0:00:15\n",
            "Epoch: 3/10... Step: 5300... Loss: 685682.937500... Val Loss: 63342.302539 Time: 0:00:17\n",
            "Epoch: 3/10... Step: 5400... Loss: 27635.289062... Val Loss: 61856.898102 Time: 0:00:19\n",
            "Epoch: 3/10... Step: 5500... Loss: 74204.812500... Val Loss: 60570.353109 Time: 0:00:22\n",
            "Epoch: 3/10... Step: 5600... Loss: 69123.023438... Val Loss: 56467.019656 Time: 0:00:24\n",
            "Model Trained and Saved\n",
            "Epoch: 3/10... Step: 5700... Loss: 74730.679688... Val Loss: 63015.976313 Time: 0:00:26\n",
            "Epoch: 3/10... Step: 5800... Loss: 45124.789062... Val Loss: 70185.977133 Time: 0:00:28\n",
            "Epoch: 3/10... Step: 5900... Loss: 85663.546875... Val Loss: 70270.509383 Time: 0:00:31\n",
            "Epoch: 3/10... Step: 6000... Loss: 114408.000000... Val Loss: 65331.705922 Time: 0:00:33\n",
            "Epoch: 3/10... Step: 6100... Loss: 30788.236328... Val Loss: 60616.850348 Time: 0:00:35\n",
            "Epoch: 3/10... Step: 6200... Loss: 32780.433594... Val Loss: 56533.319617 Time: 0:00:38\n",
            "Epoch: 3/10... Step: 6300... Loss: 50729.257812... Val Loss: 52129.639699 Time: 0:00:40\n",
            "Model Trained and Saved\n",
            "Epoch: 3/10... Step: 6400... Loss: 52894.312500... Val Loss: 62082.246773 Time: 0:00:42\n",
            "Epoch: 3/10... Step: 6500... Loss: 42592.027344... Val Loss: 62889.713820 Time: 0:00:44\n",
            "Epoch: 3/10... Step: 6600... Loss: 64730.613281... Val Loss: 61689.524695 Time: 0:00:47\n",
            "Epoch: 3/10... Step: 6700... Loss: 60467.363281... Val Loss: 56135.672719 Time: 0:00:49\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "Epoch: 4/10... Step: 6800... Loss: 43374.375000... Val Loss: 54235.372477 Time: 0:00:00\n",
            "Epoch: 4/10... Step: 6900... Loss: 49930.667969... Val Loss: 54438.629734 Time: 0:00:02\n",
            "Epoch: 4/10... Step: 7000... Loss: 95859.156250... Val Loss: 56234.989938 Time: 0:00:05\n",
            "Epoch: 4/10... Step: 7100... Loss: 16720.855469... Val Loss: 55713.393980 Time: 0:00:07\n",
            "Epoch: 4/10... Step: 7200... Loss: 59945.035156... Val Loss: 56279.385727 Time: 0:00:09\n",
            "Epoch: 4/10... Step: 7300... Loss: 212600.875000... Val Loss: 66076.825164 Time: 0:00:11\n",
            "Epoch: 4/10... Step: 7400... Loss: 129893.625000... Val Loss: 57901.123266 Time: 0:00:14\n",
            "Epoch: 4/10... Step: 7500... Loss: 74842.867188... Val Loss: 56026.143383 Time: 0:00:16\n",
            "Epoch: 4/10... Step: 7600... Loss: 57804.332031... Val Loss: 56337.976844 Time: 0:00:18\n",
            "Epoch: 4/10... Step: 7700... Loss: 101379.437500... Val Loss: 52503.502266 Time: 0:00:21\n",
            "Epoch: 4/10... Step: 7800... Loss: 77924.226562... Val Loss: 52006.094703 Time: 0:00:23\n",
            "Model Trained and Saved\n",
            "Epoch: 4/10... Step: 7900... Loss: 38264.910156... Val Loss: 50883.402527 Time: 0:00:25\n",
            "Model Trained and Saved\n",
            "Epoch: 4/10... Step: 8000... Loss: 60446.527344... Val Loss: 51529.393078 Time: 0:00:27\n",
            "Epoch: 4/10... Step: 8100... Loss: 58214.968750... Val Loss: 51612.624809 Time: 0:00:30\n",
            "Epoch: 4/10... Step: 8200... Loss: 17001.552734... Val Loss: 59648.388609 Time: 0:00:32\n",
            "Epoch: 4/10... Step: 8300... Loss: 72125.398438... Val Loss: 52543.592039 Time: 0:00:34\n",
            "Epoch: 4/10... Step: 8400... Loss: 63070.648438... Val Loss: 51306.389793 Time: 0:00:36\n",
            "Epoch: 4/10... Step: 8500... Loss: 37538.035156... Val Loss: 51756.618941 Time: 0:00:39\n",
            "Epoch: 4/10... Step: 8600... Loss: 60716.738281... Val Loss: 57058.778520 Time: 0:00:41\n",
            "Epoch: 4/10... Step: 8700... Loss: 109686.445312... Val Loss: 53380.808145 Time: 0:00:43\n",
            "Epoch: 4/10... Step: 8800... Loss: 58612.750000... Val Loss: 53923.742773 Time: 0:00:46\n",
            "Epoch: 4/10... Step: 8900... Loss: 31818.076172... Val Loss: 55055.927891 Time: 0:00:48\n",
            "Epoch: 4/10... Step: 9000... Loss: 52846.070312... Val Loss: 56716.795586 Time: 0:00:50\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "Epoch: 5/10... Step: 9100... Loss: 31703.826172... Val Loss: 50303.271852 Time: 0:00:01\n",
            "Model Trained and Saved\n",
            "Epoch: 5/10... Step: 9200... Loss: 48202.980469... Val Loss: 51640.930867 Time: 0:00:03\n",
            "Epoch: 5/10... Step: 9300... Loss: 58800.367188... Val Loss: 52544.014430 Time: 0:00:05\n",
            "Epoch: 5/10... Step: 9400... Loss: 48546.980469... Val Loss: 60682.795523 Time: 0:00:07\n",
            "Epoch: 5/10... Step: 9500... Loss: 27999.376953... Val Loss: 48755.677965 Time: 0:00:10\n",
            "Model Trained and Saved\n",
            "Epoch: 5/10... Step: 9600... Loss: 34590.000000... Val Loss: 49414.664320 Time: 0:00:12\n",
            "Epoch: 5/10... Step: 9700... Loss: 76148.601562... Val Loss: 53499.031355 Time: 0:00:14\n",
            "Epoch: 5/10... Step: 9800... Loss: 42520.437500... Val Loss: 47915.853330 Time: 0:00:17\n",
            "Model Trained and Saved\n",
            "Epoch: 5/10... Step: 9900... Loss: 50280.898438... Val Loss: 53072.319820 Time: 0:00:19\n",
            "Epoch: 5/10... Step: 10000... Loss: 22525.574219... Val Loss: 44251.525699 Time: 0:00:21\n",
            "Model Trained and Saved\n",
            "Epoch: 5/10... Step: 10100... Loss: 44011.324219... Val Loss: 52288.589766 Time: 0:00:23\n",
            "Epoch: 5/10... Step: 10200... Loss: 90403.671875... Val Loss: 47264.764395 Time: 0:00:26\n",
            "Epoch: 5/10... Step: 10300... Loss: 27769.662109... Val Loss: 45752.874301 Time: 0:00:28\n",
            "Epoch: 5/10... Step: 10400... Loss: 35826.996094... Val Loss: 50814.687359 Time: 0:00:30\n",
            "Epoch: 5/10... Step: 10500... Loss: 65175.859375... Val Loss: 48477.279293 Time: 0:00:32\n",
            "Epoch: 5/10... Step: 10600... Loss: 32927.808594... Val Loss: 47010.805125 Time: 0:00:35\n",
            "Epoch: 5/10... Step: 10700... Loss: 85147.804688... Val Loss: 49852.528621 Time: 0:00:37\n",
            "Epoch: 5/10... Step: 10800... Loss: 118616.539062... Val Loss: 48366.486725 Time: 0:00:39\n",
            "Epoch: 5/10... Step: 10900... Loss: 80779.765625... Val Loss: 50433.481250 Time: 0:00:41\n",
            "Epoch: 5/10... Step: 11000... Loss: 54858.929688... Val Loss: 49375.275879 Time: 0:00:44\n",
            "Epoch: 5/10... Step: 11100... Loss: 45627.078125... Val Loss: 46239.484684 Time: 0:00:46\n",
            "Epoch: 5/10... Step: 11200... Loss: 15644.812500... Val Loss: 56104.219711 Time: 0:00:48\n",
            "Epoch: 5/10... Step: 11300... Loss: 53885.070312... Val Loss: 51904.658734 Time: 0:00:50\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "Epoch: 6/10... Step: 11400... Loss: 57200.269531... Val Loss: 50140.578281 Time: 0:00:01\n",
            "Epoch: 6/10... Step: 11500... Loss: 39021.746094... Val Loss: 46595.092072 Time: 0:00:03\n",
            "Epoch: 6/10... Step: 11600... Loss: 40534.226562... Val Loss: 46279.936340 Time: 0:00:06\n",
            "Epoch: 6/10... Step: 11700... Loss: 34839.820312... Val Loss: 48780.046785 Time: 0:00:08\n",
            "Epoch: 6/10... Step: 11800... Loss: 32548.410156... Val Loss: 50613.102914 Time: 0:00:10\n",
            "Epoch: 6/10... Step: 11900... Loss: 134104.109375... Val Loss: 45885.318000 Time: 0:00:12\n",
            "Epoch: 6/10... Step: 12000... Loss: 45841.277344... Val Loss: 46937.751020 Time: 0:00:15\n",
            "Epoch: 6/10... Step: 12100... Loss: 262250.343750... Val Loss: 48173.641258 Time: 0:00:17\n",
            "Epoch: 6/10... Step: 12200... Loss: 63057.636719... Val Loss: 48293.078926 Time: 0:00:19\n",
            "Epoch: 6/10... Step: 12300... Loss: 39561.648438... Val Loss: 46844.645730 Time: 0:00:22\n",
            "Epoch: 6/10... Step: 12400... Loss: 83380.406250... Val Loss: 47493.215098 Time: 0:00:24\n",
            "Epoch: 6/10... Step: 12500... Loss: 40096.242188... Val Loss: 48662.727113 Time: 0:00:26\n",
            "Epoch: 6/10... Step: 12600... Loss: 69599.367188... Val Loss: 43482.637699 Time: 0:00:28\n",
            "Model Trained and Saved\n",
            "Epoch: 6/10... Step: 12700... Loss: 54778.878906... Val Loss: 44439.126133 Time: 0:00:31\n",
            "Epoch: 6/10... Step: 12800... Loss: 42847.753906... Val Loss: 43165.741014 Time: 0:00:33\n",
            "Model Trained and Saved\n",
            "Epoch: 6/10... Step: 12900... Loss: 41066.523438... Val Loss: 47943.428969 Time: 0:00:35\n",
            "Epoch: 6/10... Step: 13000... Loss: 35233.085938... Val Loss: 42676.334078 Time: 0:00:37\n",
            "Model Trained and Saved\n",
            "Epoch: 6/10... Step: 13100... Loss: 38280.996094... Val Loss: 51846.153320 Time: 0:00:40\n",
            "Epoch: 6/10... Step: 13200... Loss: 99322.007812... Val Loss: 50088.521000 Time: 0:00:42\n",
            "Epoch: 6/10... Step: 13300... Loss: 40998.957031... Val Loss: 47094.681187 Time: 0:00:44\n",
            "Epoch: 6/10... Step: 13400... Loss: 60514.972656... Val Loss: 51416.583117 Time: 0:00:47\n",
            "Epoch: 6/10... Step: 13500... Loss: 65791.531250... Val Loss: 46485.045598 Time: 0:00:49\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "Epoch: 7/10... Step: 13600... Loss: 61967.957031... Val Loss: 50264.904730 Time: 0:00:00\n",
            "Epoch: 7/10... Step: 13700... Loss: 50323.968750... Val Loss: 43751.329016 Time: 0:00:02\n",
            "Epoch: 7/10... Step: 13800... Loss: 46768.375000... Val Loss: 52759.779941 Time: 0:00:05\n",
            "Epoch: 7/10... Step: 13900... Loss: 65869.023438... Val Loss: 51647.525543 Time: 0:00:07\n",
            "Epoch: 7/10... Step: 14000... Loss: 48739.925781... Val Loss: 42796.143990 Time: 0:00:09\n",
            "Epoch: 7/10... Step: 14100... Loss: 82807.171875... Val Loss: 46431.002023 Time: 0:00:12\n",
            "Epoch: 7/10... Step: 14200... Loss: 62743.117188... Val Loss: 41892.084645 Time: 0:00:14\n",
            "Model Trained and Saved\n",
            "Epoch: 7/10... Step: 14300... Loss: 51900.750000... Val Loss: 51532.122922 Time: 0:00:16\n",
            "Epoch: 7/10... Step: 14400... Loss: 27077.593750... Val Loss: 44508.816598 Time: 0:00:19\n",
            "Epoch: 7/10... Step: 14500... Loss: 35537.074219... Val Loss: 48517.627773 Time: 0:00:21\n",
            "Epoch: 7/10... Step: 14600... Loss: 25253.146484... Val Loss: 50371.830984 Time: 0:00:23\n",
            "Epoch: 7/10... Step: 14700... Loss: 93245.179688... Val Loss: 45949.369766 Time: 0:00:25\n",
            "Epoch: 7/10... Step: 14800... Loss: 47802.343750... Val Loss: 42707.740281 Time: 0:00:28\n",
            "Epoch: 7/10... Step: 14900... Loss: 25102.464844... Val Loss: 43503.799551 Time: 0:00:30\n",
            "Epoch: 7/10... Step: 15000... Loss: 34609.023438... Val Loss: 42806.464582 Time: 0:00:32\n",
            "Epoch: 7/10... Step: 15100... Loss: 39003.445312... Val Loss: 43771.660969 Time: 0:00:34\n",
            "Epoch: 7/10... Step: 15200... Loss: 41660.433594... Val Loss: 43997.883393 Time: 0:00:37\n",
            "Epoch: 7/10... Step: 15300... Loss: 70137.656250... Val Loss: 47679.929113 Time: 0:00:39\n",
            "Epoch: 7/10... Step: 15400... Loss: 76712.851562... Val Loss: 42426.876020 Time: 0:00:41\n",
            "Epoch: 7/10... Step: 15500... Loss: 62698.437500... Val Loss: 42477.462227 Time: 0:00:44\n",
            "Epoch: 7/10... Step: 15600... Loss: 15200.402344... Val Loss: 42838.723211 Time: 0:00:46\n",
            "Epoch: 7/10... Step: 15700... Loss: 51033.515625... Val Loss: 43279.746621 Time: 0:00:48\n",
            "Epoch: 7/10... Step: 15800... Loss: 21178.628906... Val Loss: 40478.276127 Time: 0:00:50\n",
            "Model Trained and Saved\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "Epoch: 8/10... Step: 15900... Loss: 86039.906250... Val Loss: 48456.274264 Time: 0:00:01\n",
            "Epoch: 8/10... Step: 16000... Loss: 39658.019531... Val Loss: 45743.072285 Time: 0:00:03\n",
            "Epoch: 8/10... Step: 16100... Loss: 104411.140625... Val Loss: 44190.178699 Time: 0:00:05\n",
            "Epoch: 8/10... Step: 16200... Loss: 64167.660156... Val Loss: 45030.145246 Time: 0:00:07\n",
            "Epoch: 8/10... Step: 16300... Loss: 27466.619141... Val Loss: 43191.126766 Time: 0:00:10\n",
            "Epoch: 8/10... Step: 16400... Loss: 29271.136719... Val Loss: 41507.646664 Time: 0:00:12\n",
            "Epoch: 8/10... Step: 16500... Loss: 24135.666016... Val Loss: 43613.190863 Time: 0:00:14\n",
            "Epoch: 8/10... Step: 16600... Loss: 38179.941406... Val Loss: 44267.892859 Time: 0:00:17\n",
            "Epoch: 8/10... Step: 16700... Loss: 19899.138672... Val Loss: 43496.929838 Time: 0:00:19\n",
            "Epoch: 8/10... Step: 16800... Loss: 26392.136719... Val Loss: 42130.814650 Time: 0:00:21\n",
            "Epoch: 8/10... Step: 16900... Loss: 32151.039062... Val Loss: 42061.518166 Time: 0:00:23\n",
            "Epoch: 8/10... Step: 17000... Loss: 18655.011719... Val Loss: 43607.235260 Time: 0:00:26\n",
            "Epoch: 8/10... Step: 17100... Loss: 104561.414062... Val Loss: 58270.759195 Time: 0:00:28\n",
            "Epoch: 8/10... Step: 17200... Loss: 113301.195312... Val Loss: 41544.616273 Time: 0:00:30\n",
            "Epoch: 8/10... Step: 17300... Loss: 50116.363281... Val Loss: 44749.595248 Time: 0:00:32\n",
            "Epoch: 8/10... Step: 17400... Loss: 59064.437500... Val Loss: 45209.393213 Time: 0:00:35\n",
            "Epoch: 8/10... Step: 17500... Loss: 35090.222656... Val Loss: 40354.038578 Time: 0:00:37\n",
            "Model Trained and Saved\n",
            "Epoch: 8/10... Step: 17600... Loss: 31845.984375... Val Loss: 43101.338090 Time: 0:00:39\n",
            "Epoch: 8/10... Step: 17700... Loss: 47442.843750... Val Loss: 40755.739484 Time: 0:00:42\n",
            "Epoch: 8/10... Step: 17800... Loss: 65228.097656... Val Loss: 41876.387227 Time: 0:00:44\n",
            "Epoch: 8/10... Step: 17900... Loss: 25195.347656... Val Loss: 41669.155641 Time: 0:00:46\n",
            "Epoch: 8/10... Step: 18000... Loss: 75097.703125... Val Loss: 45149.262287 Time: 0:00:48\n",
            "Epoch: 8/10... Step: 18100... Loss: 26376.591797... Val Loss: 39071.707215 Time: 0:00:51\n",
            "Model Trained and Saved\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "Epoch: 9/10... Step: 18200... Loss: 36496.093750... Val Loss: 44538.676059 Time: 0:00:01\n",
            "Epoch: 9/10... Step: 18300... Loss: 35656.078125... Val Loss: 42051.141195 Time: 0:00:03\n",
            "Epoch: 9/10... Step: 18400... Loss: 31244.585938... Val Loss: 45330.758262 Time: 0:00:06\n",
            "Epoch: 9/10... Step: 18500... Loss: 19108.552734... Val Loss: 43682.491141 Time: 0:00:08\n",
            "Epoch: 9/10... Step: 18600... Loss: 28053.324219... Val Loss: 44572.206719 Time: 0:00:10\n",
            "Epoch: 9/10... Step: 18700... Loss: 47945.617188... Val Loss: 38519.286906 Time: 0:00:13\n",
            "Model Trained and Saved\n",
            "Epoch: 9/10... Step: 18800... Loss: 25483.548828... Val Loss: 45335.818934 Time: 0:00:15\n",
            "Epoch: 9/10... Step: 18900... Loss: 40865.527344... Val Loss: 46957.084219 Time: 0:00:17\n",
            "Epoch: 9/10... Step: 19000... Loss: 30200.335938... Val Loss: 46965.103086 Time: 0:00:19\n",
            "Epoch: 9/10... Step: 19100... Loss: 25058.285156... Val Loss: 38870.378359 Time: 0:00:22\n",
            "Epoch: 9/10... Step: 19200... Loss: 65853.375000... Val Loss: 37980.036340 Time: 0:00:24\n",
            "Model Trained and Saved\n",
            "Epoch: 9/10... Step: 19300... Loss: 49698.027344... Val Loss: 39772.657563 Time: 0:00:26\n",
            "Epoch: 9/10... Step: 19400... Loss: 35417.390625... Val Loss: 41471.696988 Time: 0:00:29\n",
            "Epoch: 9/10... Step: 19500... Loss: 71007.929688... Val Loss: 40743.981434 Time: 0:00:31\n",
            "Epoch: 9/10... Step: 19600... Loss: 21217.136719... Val Loss: 38323.877621 Time: 0:00:33\n",
            "Epoch: 9/10... Step: 19700... Loss: 81263.296875... Val Loss: 39863.256088 Time: 0:00:35\n",
            "Epoch: 9/10... Step: 19800... Loss: 79454.234375... Val Loss: 41596.374342 Time: 0:00:38\n",
            "Epoch: 9/10... Step: 19900... Loss: 29097.164062... Val Loss: 40948.622791 Time: 0:00:40\n",
            "Epoch: 9/10... Step: 20000... Loss: 54491.968750... Val Loss: 40451.347074 Time: 0:00:42\n",
            "Epoch: 9/10... Step: 20100... Loss: 42721.414062... Val Loss: 39760.114404 Time: 0:00:44\n",
            "Epoch: 9/10... Step: 20200... Loss: 24343.705078... Val Loss: 39955.092217 Time: 0:00:47\n",
            "Epoch: 9/10... Step: 20300... Loss: 49341.574219... Val Loss: 38544.446104 Time: 0:00:49\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "Epoch: 10/10... Step: 20400... Loss: 30030.410156... Val Loss: 43778.099871 Time: 0:00:00\n",
            "Epoch: 10/10... Step: 20500... Loss: 30245.505859... Val Loss: 43107.722314 Time: 0:00:02\n",
            "Epoch: 10/10... Step: 20600... Loss: 33768.417969... Val Loss: 38716.890857 Time: 0:00:05\n",
            "Epoch: 10/10... Step: 20700... Loss: 39743.167969... Val Loss: 40493.269543 Time: 0:00:07\n",
            "Epoch: 10/10... Step: 20800... Loss: 60827.328125... Val Loss: 38814.336289 Time: 0:00:09\n",
            "Epoch: 10/10... Step: 20900... Loss: 251046.718750... Val Loss: 39374.534684 Time: 0:00:11\n",
            "Epoch: 10/10... Step: 21000... Loss: 49788.050781... Val Loss: 40983.060475 Time: 0:00:14\n",
            "Epoch: 10/10... Step: 21100... Loss: 29121.599609... Val Loss: 39138.318146 Time: 0:00:16\n",
            "Epoch: 10/10... Step: 21200... Loss: 52912.074219... Val Loss: 43550.370848 Time: 0:00:18\n",
            "Epoch: 10/10... Step: 21300... Loss: 101790.937500... Val Loss: 40991.206984 Time: 0:00:21\n",
            "Epoch: 10/10... Step: 21400... Loss: 55837.558594... Val Loss: 39875.668437 Time: 0:00:23\n",
            "Epoch: 10/10... Step: 21500... Loss: 39203.375000... Val Loss: 37750.353891 Time: 0:00:25\n",
            "Model Trained and Saved\n",
            "Epoch: 10/10... Step: 21600... Loss: 18524.378906... Val Loss: 45049.696566 Time: 0:00:27\n",
            "Epoch: 10/10... Step: 21700... Loss: 42309.429688... Val Loss: 37995.667149 Time: 0:00:30\n",
            "Epoch: 10/10... Step: 21800... Loss: 42463.824219... Val Loss: 40211.806193 Time: 0:00:32\n",
            "Epoch: 10/10... Step: 21900... Loss: 16644.156250... Val Loss: 38898.864840 Time: 0:00:34\n",
            "Epoch: 10/10... Step: 22000... Loss: 24267.722656... Val Loss: 42579.381307 Time: 0:00:36\n",
            "Epoch: 10/10... Step: 22100... Loss: 37745.890625... Val Loss: 44098.813195 Time: 0:00:39\n",
            "Epoch: 10/10... Step: 22200... Loss: 35713.238281... Val Loss: 45041.503393 Time: 0:00:41\n",
            "Epoch: 10/10... Step: 22300... Loss: 32999.414062... Val Loss: 38491.049649 Time: 0:00:43\n",
            "Epoch: 10/10... Step: 22400... Loss: 51217.328125... Val Loss: 38998.216617 Time: 0:00:45\n",
            "Epoch: 10/10... Step: 22500... Loss: 27023.785156... Val Loss: 38051.739410 Time: 0:00:48\n",
            "Epoch: 10/10... Step: 22600... Loss: 11817.828125... Val Loss: 44232.453477 Time: 0:00:50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sCpBNmVwb-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "4b9887d4-fdd5-4336-ca01-1a914686363e"
      },
      "source": [
        "#Plotting loss\n",
        "pyplot.plot(step_list, train_loss_list, color='blue', label='training loss')\n",
        "pyplot.plot(step_list, val_loss_list, color='red', label='validating loss')\n",
        "pyplot.legend()\n",
        "pyplot.xlabel('Step')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4bEzZ2njoI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e52dc810-6d73-41db-dc6c-df2de2b835fa"
      },
      "source": [
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "net_lstm = load_model('./save/trained_lstm') #load the saved model\n",
        "\n",
        "h = net_lstm.init_hidden(batch_size)\n",
        "\n",
        "net_lstm.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net_lstm(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze()/1000)  # rounds to the nearest integer\n",
        "    original_labels = labels // 1000\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(original_labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "    # -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy of LSTM networks: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 46859.959\n",
            "Test accuracy of LSTM networks: 0.945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP7B96WslOY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_word(words):\n",
        "\n",
        "  \n",
        "    test_ints = []\n",
        "    for word in words:\n",
        "      word = word.lower()\n",
        "      test_ints.append([char_to_int[char] for char in word])\n",
        "    test_ints = pad_words(test_ints, 28)\n",
        "    return test_ints\n",
        "\n",
        "def predict(net, words):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "\n",
        "    test_ints = tokenize_word(words)\n",
        "    print(test_ints)\n",
        "    \n",
        "\n",
        "    features = np.array(test_ints)\n",
        "    \n",
        "\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    batch_size =  feature_tensor.size(0)\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    pred = torch.round(output.squeeze()/1000)\n",
        "    print(pred.data) \n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: ', (output.data))\n",
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFQDP2IJlffu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "386a6417-8d93-4523-b099-8a60abd2eb30"
      },
      "source": [
        "predict(net_lstm, ('tranquocson','tonikroos'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  5  4  8 29 15  9\n",
            "  12  7  9  8]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  9  8  6 20\n",
            "   5  9  9  7]]\n",
            "tensor([3., 3.], device='cuda:0')\n",
            "Prediction value, pre-rounding:  tensor([2974.3708, 3030.5068], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAO5s1tMlphe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}