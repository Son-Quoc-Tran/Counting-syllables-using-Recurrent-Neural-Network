{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ejwy3HKPGVO"
   },
   "source": [
    "# Counting syllables by Son Quoc Tran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSYpdvNuO62g"
   },
   "source": [
    "## ***0.Settings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RC2_PJf5FUZ6"
   },
   "outputs": [],
   "source": [
    "#import all used library\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as pyplot\n",
    "from sklearn.utils import shuffle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fOI9yIa4oF0"
   },
   "source": [
    "## ***1. Preparing data***\n",
    "The original dataset is downloaded from [CMU Pronouncing Dictionary](https://www.kaggle.com/rtatman/cmu-pronouncing-dictionary) on Kaggle. The dictionary like this can be useful for counting syllables in a word. In this part of the project, I use python to convert the data into format that I will use in training my models; the code take in the pronunciation in the dictionary and return the number of syllables the word has, and then store all the returned value in a .csv file for dataset in training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw2J2vA97XUd"
   },
   "source": [
    "### ***1.1 Importing dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXqLnu0c4NPz",
    "outputId": "f37c4417-b6f4-4d0e-c257-d7a38480ace7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135010\n"
     ]
    }
   ],
   "source": [
    "text_file = open(\"/content/cmudict.dict\", \"r\")                # I use Google Colaboratory and upload the cmudict into content folder\n",
    "lines = text_file.readlines()\n",
    "print(len(lines))                                             # investigate the length of the dataset\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqSBQKHk82eU",
    "outputId": "1d240d58-032c-4db4-9f72-d7296a2f3481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'bout B AW1 T\\n\", \"'cause K AH0 Z\\n\", \"'course K AO1 R S\\n\", \"'cuse K Y UW1 Z\\n\", \"'em AH0 M\\n\", \"'frisco F R IH1 S K OW0\\n\", \"'gain G EH1 N\\n\", \"'kay K EY1\\n\", \"'m AH0 M\\n\", \"'n AH0 N\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(lines[:10])                                             # investigate the pattern of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqRMH3PL9WZh"
   },
   "source": [
    "### ***1.2 Preprocessing data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hUkbbjjK8-70"
   },
   "outputs": [],
   "source": [
    "# use pandas to preprocess the dataset \n",
    "syllable_table = pd.DataFrame(columns=['word', 'syllable_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zIHql4T39f7H"
   },
   "outputs": [],
   "source": [
    "def have_vowels(part_of_pronunciation):\n",
    "    \"\"\"Count the number of syllables in the pronunciation of a word\n",
    "        Parameter: \n",
    "            part_of_pronunciation: part of the pronunciation extracted from the dictionary\n",
    "        Return:\n",
    "            result(boolean): whether the given part of pronuncation have vowel(s)\n",
    "    \"\"\" \n",
    "    vowels = ['U','E','O','A','I']\n",
    "    result = any(elem in part_of_pronunciation for elem in vowels)\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rVNQsXDh9_RZ"
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    split_line = line.split(' ')\n",
    "    if '(' in split_line[0]: \n",
    "        continue                           # a way to ingore multiple ways of spelling a word\n",
    "    word = split_line[0]\n",
    "    count = 0\n",
    "    for i in range(1,len(split_line)):     # iterate from the first part of pronunciation to the last one\n",
    "        if have_vowels(split_line[i]):\n",
    "          count += 1\n",
    "    df_add = pd.DataFrame([[word, count]], columns = ['word', 'syllable_count'])\n",
    "    syllable_table = syllable_table.append(df_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5r4TlESCksS",
    "outputId": "e3a17011-f151-4606-ca9c-f873ed3a3076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word syllable_count\n",
      "0    'bout              1\n",
      "1   'cause              1\n",
      "2  'course              1\n",
      "3    'cuse              1\n",
      "4      'em              1\n"
     ]
    }
   ],
   "source": [
    "print(syllable_table.head())               # to investigate the converted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eZhao5dlCxp-"
   },
   "outputs": [],
   "source": [
    "syllable_table.to_csv('/content/syllable_dict.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZgKOM2fFBhk"
   },
   "source": [
    "## ***2. Loading data for training process***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L-RJkn-FfDT"
   },
   "source": [
    "In this step, it is extremely important to shuffle the dataset. This is because the dictionary is arranged in alphabet order. If we divide the dataset into training set and validating set: about 1000 last words will be assigned into validating set. This could cause an unexpected scenario: the developed model could perform significantly bad on words beginning with 'y' or 'z' because it has never been trained to predict words like that.\n",
    "\n",
    "This is a basic rule in training deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NfGsNRHC2oP",
    "outputId": "3f6d0045-a630-46f5-bb23-7d0b88071d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the dataset .csv file...\n",
      "                 word  syllable_count\n",
      "28467      demobilize               4\n",
      "64973          lepley               2\n",
      "110973        syngman               2\n",
      "120747      wallboard               2\n",
      "53389         hultman               2\n",
      "...               ...             ...\n",
      "5169          arledge               2\n",
      "76434   multinational               5\n",
      "47829           gyles               1\n",
      "84087           pates               1\n",
      "7879             barb               1\n",
      "\n",
      "[125929 rows x 2 columns]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Load data using pandas\n",
    "path_to_dictionary = '/content/syllable_dict.csv'\n",
    "print('Parsing the dataset .csv file...')\n",
    "dictionary = pd.read_csv(path_to_dictionary)\n",
    "dictionary = shuffle(dictionary)                # shuffle the dataset\n",
    "print(dictionary)\n",
    "print('Finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg-eTN9XPWDQ"
   },
   "source": [
    "### ***2.1 Preprocess the loaded data***\n",
    "In this step, I will:\n",
    "1. load word and count into python lists and later convert intoi pytorch tensor for training.\n",
    "2. modify the values of count list to prepare for the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5fgx67uRJjgm"
   },
   "outputs": [],
   "source": [
    "words = dictionary['word']\n",
    "myWords = []\n",
    "for word in words:\n",
    "    myWords.append(str(word).lower())\n",
    "counts = dictionary['syllable_count']\n",
    "myCounts =[]\n",
    "for count in counts:\n",
    "    myCounts.append(count * 1000)                # this would hlep us heavily penalize the error of the model \n",
    "myCounts = np.array(myCounts)                    # convert myCounts into np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E8jT2OPQX3E"
   },
   "source": [
    "### ***2.2 Prepare data for training model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJmCoqchNQu3",
    "outputId": "02c4addf-1d37-4c05-c722-59c96519f3b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\": 1, ',': 2, 'e': 3, 'a': 4, 'r': 5, 'i': 6, 's': 7, 'n': 8, 'o': 9, 't': 10, 'l': 11, 'c': 12, 'd': 13, 'm': 14, 'u': 15, 'h': 16, 'g': 17, 'p': 18, 'b': 19, 'k': 20, 'y': 21, '\"': 22, 'f': 23, 'w': 24, 'v': 25, 'z': 26, 'j': 27, 'x': 28, 'q': 29, '-': 30, '.': 31, '[': 32, '1': 33, ']': 34}\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(str(myWords))\n",
    "chars = all_words.split()\n",
    "counters = Counter(chars)\n",
    "char_list = sorted(counters, key=counters.get, reverse=True)\n",
    "char_to_int = {char: ii for ii, char in enumerate(char_list, 1)}\n",
    "print(char_to_int)\n",
    "\n",
    "words_ints =[]\n",
    "for word in myWords:\n",
    "    words_ints.append([char_to_int[char] for char in str(word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrLU7tqDJyc5",
    "outputId": "68e7f1b0-ec2d-409f-d8cc-e3752b9b2e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "len_word = [len(word) for word in myWords]\n",
    "max_length = max(len_word)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi3SFPQtQ6cA"
   },
   "source": [
    "To train the model it is important to have all pieces of data in the same shape to utilize the properties of matrices, which means all words have the same length. One solution for this problem is to use padding and truncating. In this case, padding should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "v1SXmwfSLr5b"
   },
   "outputs": [],
   "source": [
    "def pad_words(words, length):\n",
    "    features = np.zeros((len(words), length), dtype=int)\n",
    "    for i, row in enumerate(words):\n",
    "        features[i, -len(row):] = np.array(row)[:length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnIlgHD9Ms74",
    "outputId": "80c5b227-e836-4db7-ed2e-92e32fb006b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  5  9 12  9 13  6\n",
      "  11  6  4  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19 11  9 14\n",
      "  19  3  5 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  4 25  6 17  4\n",
      "  10  6  8 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23 15 13\n",
      "  17  6  8 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  3\n",
      "   5 19  7  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  5  9\n",
      "  15  8  7  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  16  4 11 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 15 18  3  5 23  6\n",
      "  12  6  4 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  5  4  8\n",
      "  12 16  9 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 10  5  4 10\n",
      "  14  4  8  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 10\n",
      "   3  5 11  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  8 13  5\n",
      "   6  8 17  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 15 10  3  7  6\n",
      "   8  3  7  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  9  9 18  3\n",
      "   5 14  4  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  3  4  5 12\n",
      "  16  3  5  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19\n",
      "   4  7  7  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  14 15 10 16]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16  9 15  7\n",
      "   6  8 17  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17  4  5  4 25  4\n",
      "  17 11  6  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18\n",
      "   6 11  3 13]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  3  4  7  7\n",
      "  15 14  3 13]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  4  8\n",
      "  17 15 14  4]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 25\n",
      "   6 10  4 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  3  3\n",
      "   5  3  1  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 10  3 28]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  5 11\n",
      "   3 13 17  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14 15 11 10  6  8  4 10  6\n",
      "   9  8  4 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17\n",
      "  21 11  3  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18\n",
      "   4 10  3  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  19  4  5 19]]\n",
      "125929\n"
     ]
    }
   ],
   "source": [
    "features = pad_words(words_ints, max_length)\n",
    "print(features[-30:, :])\n",
    "print(len(features)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIAbCTD7NGzg",
    "outputId": "800f28e8-c3d7-49f9-d39f-7015d8fdc275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113336\n",
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(113336, 28) \n",
      "Validation set: \t(6296, 28) \n",
      "Test set: \t\t(6297, 28)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.9\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = int(len(features)*split_frac)\n",
    "print(split_idx)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = myCounts[:split_idx], myCounts[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_fUrMdc6Nrq2"
   },
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_wGOTOsIFOl"
   },
   "source": [
    "## ***3. Define model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apNWJluPNt0n",
    "outputId": "7bbe9e50-1aec-42ce-e35b-5dd876283f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RREC1wp_IWtT"
   },
   "source": [
    "### ***3.1 LSTM class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h7-g6fCGNvNA"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        #dropout layers\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # linear layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        # fully-connected layer\n",
    "        out = nn.functional.relu(self.fc1(lstm_out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:, -1] # get last batch of labels\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmjZzMHuIbzk"
   },
   "source": [
    "### ***3.2 Initialize model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuD9KHn8Nz6H",
    "outputId": "5accaea5-6275-4898-bb9e-353bc14e071d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM \n",
      " LSTM(\n",
      "  (embedding): Embedding(35, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(char_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net_lstm = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print('LSTM \\n', net_lstm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fxdeINRYN1Rf"
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.0005\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net_lstm.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CeRm0BLIgkt"
   },
   "source": [
    "## ***4. Training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "PMmDqxGxOPKW"
   },
   "outputs": [],
   "source": [
    "#functions used for training models\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "\n",
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "raH1U-6LOQzH",
    "outputId": "305ea867-4c7c-4d90-96ef-e3dd930d0d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "Epoch: 1/10... Step: 100... Loss: 6530050.500000... Val Loss: 6532365.168000 Time: 0:00:01\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 200... Loss: 4446243.500000... Val Loss: 5597902.550000 Time: 0:00:02\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 300... Loss: 3428387.500000... Val Loss: 4325361.906000 Time: 0:00:03\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 400... Loss: 2037830.875000... Val Loss: 2943450.354000 Time: 0:00:05\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 500... Loss: 2574501.500000... Val Loss: 1708983.176000 Time: 0:00:06\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 600... Loss: 1098633.750000... Val Loss: 1020432.528000 Time: 0:00:07\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 700... Loss: 684882.562500... Val Loss: 981543.163750 Time: 0:00:08\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 800... Loss: 1031115.812500... Val Loss: 981203.221750 Time: 0:00:09\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 900... Loss: 660199.437500... Val Loss: 662237.989500 Time: 0:00:10\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1000... Loss: 148819.109375... Val Loss: 388951.758125 Time: 0:00:11\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1100... Loss: 329877.562500... Val Loss: 261942.381500 Time: 0:00:12\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1200... Loss: 173373.937500... Val Loss: 195851.817656 Time: 0:00:13\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1300... Loss: 123517.328125... Val Loss: 166821.989875 Time: 0:00:14\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1400... Loss: 80360.734375... Val Loss: 133334.818812 Time: 0:00:16\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1500... Loss: 149702.187500... Val Loss: 123971.565172 Time: 0:00:17\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1600... Loss: 145632.859375... Val Loss: 116014.924469 Time: 0:00:18\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1700... Loss: 99330.281250... Val Loss: 104579.371687 Time: 0:00:19\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1800... Loss: 98768.226562... Val Loss: 98958.285953 Time: 0:00:20\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 1900... Loss: 55872.507812... Val Loss: 125804.778500 Time: 0:00:21\n",
      "Epoch: 1/10... Step: 2000... Loss: 69978.664062... Val Loss: 96856.106656 Time: 0:00:22\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 2100... Loss: 98343.570312... Val Loss: 83766.784125 Time: 0:00:23\n",
      "Model Trained and Saved\n",
      "Epoch: 1/10... Step: 2200... Loss: 77907.226562... Val Loss: 99615.074047 Time: 0:00:24\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "Epoch: 2/10... Step: 2300... Loss: 119878.515625... Val Loss: 86627.929969 Time: 0:00:00\n",
      "Epoch: 2/10... Step: 2400... Loss: 144440.171875... Val Loss: 95984.500000 Time: 0:00:01\n",
      "Epoch: 2/10... Step: 2500... Loss: 51795.097656... Val Loss: 84651.389063 Time: 0:00:02\n",
      "Epoch: 2/10... Step: 2600... Loss: 64593.464844... Val Loss: 67969.449250 Time: 0:00:04\n",
      "Model Trained and Saved\n",
      "Epoch: 2/10... Step: 2700... Loss: 65398.636719... Val Loss: 71904.386555 Time: 0:00:05\n",
      "Epoch: 2/10... Step: 2800... Loss: 93443.984375... Val Loss: 74124.998758 Time: 0:00:06\n",
      "Epoch: 2/10... Step: 2900... Loss: 109424.898438... Val Loss: 68913.108953 Time: 0:00:07\n",
      "Epoch: 2/10... Step: 3000... Loss: 63129.519531... Val Loss: 64028.908273 Time: 0:00:08\n",
      "Model Trained and Saved\n",
      "Epoch: 2/10... Step: 3100... Loss: 75964.421875... Val Loss: 70776.866094 Time: 0:00:09\n",
      "Epoch: 2/10... Step: 3200... Loss: 61479.937500... Val Loss: 72386.014133 Time: 0:00:10\n",
      "Epoch: 2/10... Step: 3300... Loss: 48952.539062... Val Loss: 75448.288250 Time: 0:00:11\n",
      "Epoch: 2/10... Step: 3400... Loss: 82789.765625... Val Loss: 67914.779172 Time: 0:00:12\n",
      "Epoch: 2/10... Step: 3500... Loss: 187176.218750... Val Loss: 65244.563156 Time: 0:00:13\n",
      "Epoch: 2/10... Step: 3600... Loss: 39017.625000... Val Loss: 77664.729031 Time: 0:00:14\n",
      "Epoch: 2/10... Step: 3700... Loss: 39592.863281... Val Loss: 61497.584109 Time: 0:00:15\n",
      "Model Trained and Saved\n",
      "Epoch: 2/10... Step: 3800... Loss: 29406.798828... Val Loss: 69804.793531 Time: 0:00:17\n",
      "Epoch: 2/10... Step: 3900... Loss: 92956.367188... Val Loss: 63591.014234 Time: 0:00:18\n",
      "Epoch: 2/10... Step: 4000... Loss: 47282.484375... Val Loss: 60666.513164 Time: 0:00:19\n",
      "Model Trained and Saved\n",
      "Epoch: 2/10... Step: 4100... Loss: 22072.218750... Val Loss: 61566.304078 Time: 0:00:20\n",
      "Epoch: 2/10... Step: 4200... Loss: 19590.746094... Val Loss: 58151.297586 Time: 0:00:21\n",
      "Model Trained and Saved\n",
      "Epoch: 2/10... Step: 4300... Loss: 62523.613281... Val Loss: 60166.104320 Time: 0:00:22\n",
      "Epoch: 2/10... Step: 4400... Loss: 98909.476562... Val Loss: 56176.265719 Time: 0:00:23\n",
      "Model Trained and Saved\n",
      "Epoch: 2/10... Step: 4500... Loss: 82803.210938... Val Loss: 69111.690906 Time: 0:00:24\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "Epoch: 3/10... Step: 4600... Loss: 97854.585938... Val Loss: 68216.834937 Time: 0:00:01\n",
      "Epoch: 3/10... Step: 4700... Loss: 64894.578125... Val Loss: 64720.911359 Time: 0:00:02\n",
      "Epoch: 3/10... Step: 4800... Loss: 119755.210938... Val Loss: 56417.393219 Time: 0:00:03\n",
      "Epoch: 3/10... Step: 4900... Loss: 54025.132812... Val Loss: 69518.606344 Time: 0:00:04\n",
      "Epoch: 3/10... Step: 5000... Loss: 38120.003906... Val Loss: 57572.524172 Time: 0:00:05\n",
      "Epoch: 3/10... Step: 5100... Loss: 80366.976562... Val Loss: 58502.416367 Time: 0:00:06\n",
      "Epoch: 3/10... Step: 5200... Loss: 88346.726562... Val Loss: 58291.640469 Time: 0:00:07\n",
      "Epoch: 3/10... Step: 5300... Loss: 67452.429688... Val Loss: 61686.743055 Time: 0:00:08\n",
      "Epoch: 3/10... Step: 5400... Loss: 59606.902344... Val Loss: 64129.646180 Time: 0:00:09\n",
      "Epoch: 3/10... Step: 5500... Loss: 62607.250000... Val Loss: 59745.999156 Time: 0:00:10\n",
      "Epoch: 3/10... Step: 5600... Loss: 44336.980469... Val Loss: 56585.758891 Time: 0:00:11\n",
      "Epoch: 3/10... Step: 5700... Loss: 42557.660156... Val Loss: 57587.741156 Time: 0:00:12\n",
      "Epoch: 3/10... Step: 5800... Loss: 32678.343750... Val Loss: 52961.044570 Time: 0:00:14\n",
      "Model Trained and Saved\n",
      "Epoch: 3/10... Step: 5900... Loss: 70383.945312... Val Loss: 57296.621836 Time: 0:00:15\n",
      "Epoch: 3/10... Step: 6000... Loss: 68111.679688... Val Loss: 54427.202297 Time: 0:00:16\n",
      "Epoch: 3/10... Step: 6100... Loss: 46802.230469... Val Loss: 52003.738164 Time: 0:00:17\n",
      "Model Trained and Saved\n",
      "Epoch: 3/10... Step: 6200... Loss: 26167.423828... Val Loss: 53730.138516 Time: 0:00:18\n",
      "Epoch: 3/10... Step: 6300... Loss: 84551.757812... Val Loss: 54955.988234 Time: 0:00:19\n",
      "Epoch: 3/10... Step: 6400... Loss: 52508.699219... Val Loss: 52760.561414 Time: 0:00:20\n",
      "Epoch: 3/10... Step: 6500... Loss: 67391.320312... Val Loss: 56902.314586 Time: 0:00:21\n",
      "Epoch: 3/10... Step: 6600... Loss: 63313.582031... Val Loss: 60448.087469 Time: 0:00:22\n",
      "Epoch: 3/10... Step: 6700... Loss: 54162.125000... Val Loss: 55592.968359 Time: 0:00:23\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "Epoch: 4/10... Step: 6800... Loss: 47541.328125... Val Loss: 65728.785984 Time: 0:00:00\n",
      "Epoch: 4/10... Step: 6900... Loss: 34478.675781... Val Loss: 53613.885281 Time: 0:00:01\n",
      "Epoch: 4/10... Step: 7000... Loss: 28661.949219... Val Loss: 55538.134070 Time: 0:00:02\n",
      "Epoch: 4/10... Step: 7100... Loss: 26778.164062... Val Loss: 63295.291258 Time: 0:00:03\n",
      "Epoch: 4/10... Step: 7200... Loss: 44170.617188... Val Loss: 50786.567043 Time: 0:00:04\n",
      "Model Trained and Saved\n",
      "Epoch: 4/10... Step: 7300... Loss: 44705.875000... Val Loss: 54328.358719 Time: 0:00:06\n",
      "Epoch: 4/10... Step: 7400... Loss: 54489.937500... Val Loss: 53325.811008 Time: 0:00:07\n",
      "Epoch: 4/10... Step: 7500... Loss: 27798.859375... Val Loss: 48510.403148 Time: 0:00:08\n",
      "Model Trained and Saved\n",
      "Epoch: 4/10... Step: 7600... Loss: 60721.625000... Val Loss: 53856.768531 Time: 0:00:09\n",
      "Epoch: 4/10... Step: 7700... Loss: 34593.980469... Val Loss: 49370.308055 Time: 0:00:10\n",
      "Epoch: 4/10... Step: 7800... Loss: 41819.035156... Val Loss: 45210.298922 Time: 0:00:11\n",
      "Model Trained and Saved\n",
      "Epoch: 4/10... Step: 7900... Loss: 39112.507812... Val Loss: 57393.984781 Time: 0:00:12\n",
      "Epoch: 4/10... Step: 8000... Loss: 92252.656250... Val Loss: 48502.739484 Time: 0:00:13\n",
      "Epoch: 4/10... Step: 8100... Loss: 49147.089844... Val Loss: 46959.339102 Time: 0:00:15\n",
      "Epoch: 4/10... Step: 8200... Loss: 60349.812500... Val Loss: 50017.961414 Time: 0:00:16\n",
      "Epoch: 4/10... Step: 8300... Loss: 47721.832031... Val Loss: 53090.924102 Time: 0:00:17\n",
      "Epoch: 4/10... Step: 8400... Loss: 26720.531250... Val Loss: 54131.941734 Time: 0:00:18\n",
      "Epoch: 4/10... Step: 8500... Loss: 47565.062500... Val Loss: 56707.712523 Time: 0:00:19\n",
      "Epoch: 4/10... Step: 8600... Loss: 51799.453125... Val Loss: 59981.563781 Time: 0:00:20\n",
      "Epoch: 4/10... Step: 8700... Loss: 71790.679688... Val Loss: 48667.852719 Time: 0:00:21\n",
      "Epoch: 4/10... Step: 8800... Loss: 49237.937500... Val Loss: 52632.230273 Time: 0:00:22\n",
      "Epoch: 4/10... Step: 8900... Loss: 44228.980469... Val Loss: 46186.873703 Time: 0:00:23\n",
      "Epoch: 4/10... Step: 9000... Loss: 46195.207031... Val Loss: 46287.635687 Time: 0:00:24\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "Epoch: 5/10... Step: 9100... Loss: 61772.253906... Val Loss: 50764.234930 Time: 0:00:00\n",
      "Epoch: 5/10... Step: 9200... Loss: 47899.808594... Val Loss: 48937.434281 Time: 0:00:01\n",
      "Epoch: 5/10... Step: 9300... Loss: 19078.507812... Val Loss: 45157.379328 Time: 0:00:02\n",
      "Model Trained and Saved\n",
      "Epoch: 5/10... Step: 9400... Loss: 74776.515625... Val Loss: 44173.253129 Time: 0:00:04\n",
      "Model Trained and Saved\n",
      "Epoch: 5/10... Step: 9500... Loss: 83699.109375... Val Loss: 49451.465645 Time: 0:00:05\n",
      "Epoch: 5/10... Step: 9600... Loss: 66347.656250... Val Loss: 48329.166125 Time: 0:00:06\n",
      "Epoch: 5/10... Step: 9700... Loss: 98174.507812... Val Loss: 53316.612562 Time: 0:00:07\n",
      "Epoch: 5/10... Step: 9800... Loss: 48407.457031... Val Loss: 46608.276930 Time: 0:00:08\n",
      "Epoch: 5/10... Step: 9900... Loss: 43257.113281... Val Loss: 48945.160898 Time: 0:00:09\n",
      "Epoch: 5/10... Step: 10000... Loss: 71598.648438... Val Loss: 44645.237918 Time: 0:00:10\n",
      "Epoch: 5/10... Step: 10100... Loss: 87226.757812... Val Loss: 43442.631875 Time: 0:00:11\n",
      "Model Trained and Saved\n",
      "Epoch: 5/10... Step: 10200... Loss: 30831.910156... Val Loss: 42461.499984 Time: 0:00:12\n",
      "Model Trained and Saved\n",
      "Epoch: 5/10... Step: 10300... Loss: 51720.609375... Val Loss: 45587.716805 Time: 0:00:13\n",
      "Epoch: 5/10... Step: 10400... Loss: 54191.265625... Val Loss: 49878.763039 Time: 0:00:14\n",
      "Epoch: 5/10... Step: 10500... Loss: 96733.164062... Val Loss: 46241.079641 Time: 0:00:16\n",
      "Epoch: 5/10... Step: 10600... Loss: 56652.980469... Val Loss: 51476.370320 Time: 0:00:17\n",
      "Epoch: 5/10... Step: 10700... Loss: 31819.544922... Val Loss: 48806.263215 Time: 0:00:18\n",
      "Epoch: 5/10... Step: 10800... Loss: 73647.734375... Val Loss: 44887.683090 Time: 0:00:19\n",
      "Epoch: 5/10... Step: 10900... Loss: 40276.519531... Val Loss: 52846.768203 Time: 0:00:20\n",
      "Epoch: 5/10... Step: 11000... Loss: 42213.898438... Val Loss: 44989.044609 Time: 0:00:21\n",
      "Epoch: 5/10... Step: 11100... Loss: 53073.488281... Val Loss: 50520.613500 Time: 0:00:22\n",
      "Epoch: 5/10... Step: 11200... Loss: 9534.479492... Val Loss: 45867.690148 Time: 0:00:23\n",
      "Epoch: 5/10... Step: 11300... Loss: 31337.615234... Val Loss: 43448.709129 Time: 0:00:24\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "Epoch: 6/10... Step: 11400... Loss: 33800.109375... Val Loss: 46616.280437 Time: 0:00:01\n",
      "Epoch: 6/10... Step: 11500... Loss: 46484.515625... Val Loss: 44422.247199 Time: 0:00:02\n",
      "Epoch: 6/10... Step: 11600... Loss: 58247.250000... Val Loss: 45014.612305 Time: 0:00:03\n",
      "Epoch: 6/10... Step: 11700... Loss: 33466.375000... Val Loss: 52403.835719 Time: 0:00:04\n",
      "Epoch: 6/10... Step: 11800... Loss: 31308.369141... Val Loss: 42798.675242 Time: 0:00:05\n",
      "Epoch: 6/10... Step: 11900... Loss: 64221.757812... Val Loss: 43621.282727 Time: 0:00:06\n",
      "Epoch: 6/10... Step: 12000... Loss: 53260.929688... Val Loss: 45774.874340 Time: 0:00:07\n",
      "Epoch: 6/10... Step: 12100... Loss: 41201.894531... Val Loss: 51176.924594 Time: 0:00:08\n",
      "Epoch: 6/10... Step: 12200... Loss: 27249.904297... Val Loss: 46093.233852 Time: 0:00:09\n",
      "Epoch: 6/10... Step: 12300... Loss: 25623.759766... Val Loss: 55629.205602 Time: 0:00:10\n",
      "Epoch: 6/10... Step: 12400... Loss: 147715.500000... Val Loss: 45995.634312 Time: 0:00:11\n",
      "Epoch: 6/10... Step: 12500... Loss: 39348.691406... Val Loss: 46523.131695 Time: 0:00:13\n",
      "Epoch: 6/10... Step: 12600... Loss: 70468.992188... Val Loss: 47107.507652 Time: 0:00:14\n",
      "Epoch: 6/10... Step: 12700... Loss: 69200.992188... Val Loss: 47263.188957 Time: 0:00:15\n",
      "Epoch: 6/10... Step: 12800... Loss: 69933.210938... Val Loss: 42742.849229 Time: 0:00:16\n",
      "Epoch: 6/10... Step: 12900... Loss: 23435.511719... Val Loss: 44235.810793 Time: 0:00:17\n",
      "Epoch: 6/10... Step: 13000... Loss: 45413.160156... Val Loss: 40460.011711 Time: 0:00:18\n",
      "Model Trained and Saved\n",
      "Epoch: 6/10... Step: 13100... Loss: 58641.703125... Val Loss: 49533.348832 Time: 0:00:19\n",
      "Epoch: 6/10... Step: 13200... Loss: 50152.769531... Val Loss: 49438.176027 Time: 0:00:20\n",
      "Epoch: 6/10... Step: 13300... Loss: 29761.458984... Val Loss: 41911.679336 Time: 0:00:21\n",
      "Epoch: 6/10... Step: 13400... Loss: 45133.539062... Val Loss: 44964.463664 Time: 0:00:22\n",
      "Epoch: 6/10... Step: 13500... Loss: 35048.347656... Val Loss: 44199.472578 Time: 0:00:24\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "Epoch: 7/10... Step: 13600... Loss: 22870.535156... Val Loss: 47066.570824 Time: 0:00:00\n",
      "Epoch: 7/10... Step: 13700... Loss: 17474.361328... Val Loss: 42700.531324 Time: 0:00:01\n",
      "Epoch: 7/10... Step: 13800... Loss: 132567.656250... Val Loss: 48324.868020 Time: 0:00:02\n",
      "Epoch: 7/10... Step: 13900... Loss: 42248.914062... Val Loss: 43782.513535 Time: 0:00:03\n",
      "Epoch: 7/10... Step: 14000... Loss: 109676.554688... Val Loss: 43777.314047 Time: 0:00:05\n",
      "Epoch: 7/10... Step: 14100... Loss: 60401.027344... Val Loss: 48779.098348 Time: 0:00:06\n",
      "Epoch: 7/10... Step: 14200... Loss: 104015.679688... Val Loss: 49003.815051 Time: 0:00:07\n",
      "Epoch: 7/10... Step: 14300... Loss: 28726.080078... Val Loss: 43430.715297 Time: 0:00:08\n",
      "Epoch: 7/10... Step: 14400... Loss: 24188.269531... Val Loss: 50551.109746 Time: 0:00:09\n",
      "Epoch: 7/10... Step: 14500... Loss: 45333.054688... Val Loss: 46756.419961 Time: 0:00:10\n",
      "Epoch: 7/10... Step: 14600... Loss: 26097.263672... Val Loss: 46115.093234 Time: 0:00:11\n",
      "Epoch: 7/10... Step: 14700... Loss: 62731.796875... Val Loss: 45547.481908 Time: 0:00:12\n",
      "Epoch: 7/10... Step: 14800... Loss: 60588.597656... Val Loss: 43107.482359 Time: 0:00:14\n",
      "Epoch: 7/10... Step: 14900... Loss: 50285.417969... Val Loss: 46892.620133 Time: 0:00:15\n",
      "Epoch: 7/10... Step: 15000... Loss: 70476.046875... Val Loss: 45688.155957 Time: 0:00:16\n",
      "Epoch: 7/10... Step: 15100... Loss: 47778.562500... Val Loss: 44800.382605 Time: 0:00:17\n",
      "Epoch: 7/10... Step: 15200... Loss: 56278.953125... Val Loss: 43687.834246 Time: 0:00:18\n",
      "Epoch: 7/10... Step: 15300... Loss: 62006.605469... Val Loss: 41831.881141 Time: 0:00:19\n",
      "Epoch: 7/10... Step: 15400... Loss: 21605.509766... Val Loss: 42127.084035 Time: 0:00:20\n",
      "Epoch: 7/10... Step: 15500... Loss: 54122.707031... Val Loss: 39757.602271 Time: 0:00:21\n",
      "Model Trained and Saved\n",
      "Epoch: 7/10... Step: 15600... Loss: 41131.746094... Val Loss: 39045.658799 Time: 0:00:22\n",
      "Model Trained and Saved\n",
      "Epoch: 7/10... Step: 15700... Loss: 87429.820312... Val Loss: 42234.427766 Time: 0:00:23\n",
      "Epoch: 7/10... Step: 15800... Loss: 30595.000000... Val Loss: 39891.133914 Time: 0:00:24\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "Epoch: 8/10... Step: 15900... Loss: 19858.181641... Val Loss: 40999.041902 Time: 0:00:00\n",
      "Epoch: 8/10... Step: 16000... Loss: 55855.917969... Val Loss: 40620.387687 Time: 0:00:01\n",
      "Epoch: 8/10... Step: 16100... Loss: 19562.017578... Val Loss: 37314.598223 Time: 0:00:02\n",
      "Model Trained and Saved\n",
      "Epoch: 8/10... Step: 16200... Loss: 46119.679688... Val Loss: 42376.575980 Time: 0:00:04\n",
      "Epoch: 8/10... Step: 16300... Loss: 39602.609375... Val Loss: 41049.000285 Time: 0:00:05\n",
      "Epoch: 8/10... Step: 16400... Loss: 52695.707031... Val Loss: 45219.004090 Time: 0:00:06\n",
      "Epoch: 8/10... Step: 16500... Loss: 25798.785156... Val Loss: 40735.085250 Time: 0:00:07\n",
      "Epoch: 8/10... Step: 16600... Loss: 47257.988281... Val Loss: 41059.010145 Time: 0:00:08\n",
      "Epoch: 8/10... Step: 16700... Loss: 14146.683594... Val Loss: 42259.289363 Time: 0:00:09\n",
      "Epoch: 8/10... Step: 16800... Loss: 13480.559570... Val Loss: 39661.027250 Time: 0:00:10\n",
      "Epoch: 8/10... Step: 16900... Loss: 29208.156250... Val Loss: 43237.064988 Time: 0:00:11\n",
      "Epoch: 8/10... Step: 17000... Loss: 38992.113281... Val Loss: 44112.615914 Time: 0:00:13\n",
      "Epoch: 8/10... Step: 17100... Loss: 48423.738281... Val Loss: 39764.111271 Time: 0:00:14\n",
      "Epoch: 8/10... Step: 17200... Loss: 55926.667969... Val Loss: 47439.561891 Time: 0:00:15\n",
      "Epoch: 8/10... Step: 17300... Loss: 41844.089844... Val Loss: 40447.310367 Time: 0:00:16\n",
      "Epoch: 8/10... Step: 17400... Loss: 19662.919922... Val Loss: 41265.637863 Time: 0:00:17\n",
      "Epoch: 8/10... Step: 17500... Loss: 72136.195312... Val Loss: 41233.060674 Time: 0:00:18\n",
      "Epoch: 8/10... Step: 17600... Loss: 17988.958984... Val Loss: 42232.971070 Time: 0:00:19\n",
      "Epoch: 8/10... Step: 17700... Loss: 64841.832031... Val Loss: 47622.387406 Time: 0:00:20\n",
      "Epoch: 8/10... Step: 17800... Loss: 54467.039062... Val Loss: 42816.272652 Time: 0:00:21\n",
      "Epoch: 8/10... Step: 17900... Loss: 33144.722656... Val Loss: 38742.356016 Time: 0:00:23\n",
      "Epoch: 8/10... Step: 18000... Loss: 55728.257812... Val Loss: 43679.641473 Time: 0:00:24\n",
      "Epoch: 8/10... Step: 18100... Loss: 33737.078125... Val Loss: 42639.893910 Time: 0:00:25\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "Epoch: 9/10... Step: 18200... Loss: 57457.699219... Val Loss: 39019.767137 Time: 0:00:01\n",
      "Epoch: 9/10... Step: 18300... Loss: 49279.667969... Val Loss: 43599.399016 Time: 0:00:02\n",
      "Epoch: 9/10... Step: 18400... Loss: 83026.609375... Val Loss: 45249.838070 Time: 0:00:03\n",
      "Epoch: 9/10... Step: 18500... Loss: 79380.125000... Val Loss: 40476.508836 Time: 0:00:04\n",
      "Epoch: 9/10... Step: 18600... Loss: 48436.207031... Val Loss: 43626.159242 Time: 0:00:05\n",
      "Epoch: 9/10... Step: 18700... Loss: 41945.503906... Val Loss: 40695.947611 Time: 0:00:06\n",
      "Epoch: 9/10... Step: 18800... Loss: 57454.949219... Val Loss: 40344.284395 Time: 0:00:07\n",
      "Epoch: 9/10... Step: 18900... Loss: 72169.250000... Val Loss: 40264.151906 Time: 0:00:08\n",
      "Epoch: 9/10... Step: 19000... Loss: 17209.980469... Val Loss: 38065.868787 Time: 0:00:10\n",
      "Epoch: 9/10... Step: 19100... Loss: 19846.365234... Val Loss: 50120.075133 Time: 0:00:11\n",
      "Epoch: 9/10... Step: 19200... Loss: 17557.296875... Val Loss: 40730.533012 Time: 0:00:12\n",
      "Epoch: 9/10... Step: 19300... Loss: 47722.707031... Val Loss: 38323.747686 Time: 0:00:13\n",
      "Epoch: 9/10... Step: 19400... Loss: 133278.375000... Val Loss: 43279.793738 Time: 0:00:14\n",
      "Epoch: 9/10... Step: 19500... Loss: 61759.867188... Val Loss: 38555.939246 Time: 0:00:15\n",
      "Epoch: 9/10... Step: 19600... Loss: 40233.445312... Val Loss: 37825.022533 Time: 0:00:16\n",
      "Epoch: 9/10... Step: 19700... Loss: 50674.464844... Val Loss: 38466.808084 Time: 0:00:17\n",
      "Epoch: 9/10... Step: 19800... Loss: 27771.876953... Val Loss: 36785.546307 Time: 0:00:18\n",
      "Model Trained and Saved\n",
      "Epoch: 9/10... Step: 19900... Loss: 49651.140625... Val Loss: 41292.357602 Time: 0:00:19\n",
      "Epoch: 9/10... Step: 20000... Loss: 69885.851562... Val Loss: 39674.302914 Time: 0:00:21\n",
      "Epoch: 9/10... Step: 20100... Loss: 66543.570312... Val Loss: 39461.042982 Time: 0:00:22\n",
      "Epoch: 9/10... Step: 20200... Loss: 35004.070312... Val Loss: 40724.515706 Time: 0:00:23\n",
      "Epoch: 9/10... Step: 20300... Loss: 44836.527344... Val Loss: 43520.080766 Time: 0:00:24\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "Epoch: 10/10... Step: 20400... Loss: 75340.500000... Val Loss: 41817.958793 Time: 0:00:00\n",
      "Epoch: 10/10... Step: 20500... Loss: 49658.742188... Val Loss: 40600.234047 Time: 0:00:01\n",
      "Epoch: 10/10... Step: 20600... Loss: 31273.552734... Val Loss: 39972.059145 Time: 0:00:02\n",
      "Epoch: 10/10... Step: 20700... Loss: 25619.279297... Val Loss: 40296.033344 Time: 0:00:03\n",
      "Epoch: 10/10... Step: 20800... Loss: 24529.474609... Val Loss: 36409.687486 Time: 0:00:05\n",
      "Model Trained and Saved\n",
      "Epoch: 10/10... Step: 20900... Loss: 34555.257812... Val Loss: 43060.998160 Time: 0:00:06\n",
      "Epoch: 10/10... Step: 21000... Loss: 18029.927734... Val Loss: 40286.753119 Time: 0:00:07\n",
      "Epoch: 10/10... Step: 21100... Loss: 16529.820312... Val Loss: 41489.997793 Time: 0:00:08\n",
      "Epoch: 10/10... Step: 21200... Loss: 35083.761719... Val Loss: 39330.212504 Time: 0:00:09\n",
      "Epoch: 10/10... Step: 21300... Loss: 45264.972656... Val Loss: 40311.129947 Time: 0:00:10\n",
      "Epoch: 10/10... Step: 21400... Loss: 23260.902344... Val Loss: 40361.564361 Time: 0:00:11\n",
      "Epoch: 10/10... Step: 21500... Loss: 23627.412109... Val Loss: 37830.884547 Time: 0:00:12\n",
      "Epoch: 10/10... Step: 21600... Loss: 15506.520508... Val Loss: 38388.662057 Time: 0:00:13\n",
      "Epoch: 10/10... Step: 21700... Loss: 62198.667969... Val Loss: 41059.487977 Time: 0:00:14\n",
      "Epoch: 10/10... Step: 21800... Loss: 85217.515625... Val Loss: 43832.165480 Time: 0:00:16\n",
      "Epoch: 10/10... Step: 21900... Loss: 44390.074219... Val Loss: 40519.393568 Time: 0:00:17\n",
      "Epoch: 10/10... Step: 22000... Loss: 31685.769531... Val Loss: 42701.364338 Time: 0:00:18\n",
      "Epoch: 10/10... Step: 22100... Loss: 22703.574219... Val Loss: 40905.204785 Time: 0:00:19\n",
      "Epoch: 10/10... Step: 22200... Loss: 22328.302734... Val Loss: 39997.401276 Time: 0:00:20\n",
      "Epoch: 10/10... Step: 22300... Loss: 32782.222656... Val Loss: 39830.165490 Time: 0:00:21\n",
      "Epoch: 10/10... Step: 22400... Loss: 46727.527344... Val Loss: 38171.290113 Time: 0:00:22\n",
      "Epoch: 10/10... Step: 22500... Loss: 48644.355469... Val Loss: 44478.806992 Time: 0:00:23\n",
      "Epoch: 10/10... Step: 22600... Loss: 42670.730469... Val Loss: 38976.265527 Time: 0:00:24\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "#lists used for plotting losses\n",
    "step_list = list()\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net_lstm.cuda()\n",
    "\n",
    "#use min_loss to track the best model throughout the training loop\n",
    "min_val_loss = math.inf\n",
    "\n",
    "net_lstm.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # initialize hidden state\n",
    "    h = net_lstm.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net_lstm.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, _ = net_lstm(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net_lstm.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Get validation loss\n",
    "            val_h = net_lstm.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net_lstm.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net_lstm(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "\n",
    "            net_lstm.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                  \"Time: {:}\".format(elapsed))\n",
    "            if np.mean(val_losses) < min_val_loss:\n",
    "                min_val_loss = np.mean(val_losses)\n",
    "                save_model('./save/trained_lstm', net_lstm)\n",
    "                print('Model Trained and Saved')\n",
    "            \n",
    "            # For plotting loss using pyplot\n",
    "            step_list.append(counter)\n",
    "            train_loss_list.append(loss.item())\n",
    "            val_loss_list.append(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "JseCQ8JXOUGu",
    "outputId": "e389d400-d46d-4e70-bbbd-48657e7247aa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting loss\n",
    "pyplot.plot(step_list, train_loss_list, color='blue', label='training loss')\n",
    "pyplot.plot(step_list, val_loss_list, color='red', label='validating loss')\n",
    "pyplot.legend()\n",
    "pyplot.xlabel('Step')\n",
    "pyplot.ylabel('Loss')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEQ-N0wXIu8w"
   },
   "source": [
    "## ***5. Testing the performance of the model***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OKhkXueOVgv",
    "outputId": "343b2084-e8b9-4923-a9d7-2ac038848287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 38150.624\n",
      "Test accuracy of LSTM networks: 0.952\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "ints = []\n",
    "predictions= []\n",
    "orig_labels = []\n",
    "\n",
    "net_lstm = load_model('./save/trained_lstm') #load the saved model\n",
    "\n",
    "h = net_lstm.init_hidden(batch_size)\n",
    "\n",
    "net_lstm.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    ints.extend(inputs.to('cpu').tolist())\n",
    "\n",
    "    # get predicted outputs\n",
    "    output, h = net_lstm(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    pred = torch.round(output.squeeze()/1000)  # rounds to the nearest integer\n",
    "    original_labels = labels // 1000\n",
    "\n",
    "    predictions.extend(pred)\n",
    "    orig_labels.extend(original_labels)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(original_labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "    # -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy of LSTM networks: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZvwoJbFZFy8v"
   },
   "outputs": [],
   "source": [
    "int_to_char = {v: k for k, v in char_to_int.items()}\n",
    "def convert_ints_to_word(ints):\n",
    "    word = ''\n",
    "    for num in ints:\n",
    "        if num != 0:\n",
    "            word += int_to_char[num]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0qL_OFX_Fz-U"
   },
   "outputs": [],
   "source": [
    "error_list = []\n",
    "ints = list(ints)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] != orig_labels[i]:\n",
    "        error_word = convert_ints_to_word(ints[i])\n",
    "        error_list.append(\"{}\\t{}\\t{}\".format(error_word, orig_labels[i], predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XspUzllbJAP8"
   },
   "source": [
    "### ***5.1 Print out the words with false predictions***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjxMKW_1FlsD"
   },
   "source": [
    "***Note:*** original labels first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQnBusIxF3Ns",
    "outputId": "6b60c2a0-8543-4d87-8e9a-2b8e68100e05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giggly\t3\t2.0\n",
      "wineberg\t2\t3.0\n",
      "alyeska's\t3\t4.0\n",
      "minefields\t2\t3.0\n",
      "bichler\t3\t2.0\n",
      "dieterich\t3\t4.0\n",
      "cnet\t2\t1.0\n",
      "cacld\t2\t1.0\n",
      "rademacher\t4\t3.0\n",
      "kouyate\t3\t2.0\n",
      "weyenberg\t2\t3.0\n",
      "gracias\t3\t2.0\n",
      "bachailian\t4\t3.0\n",
      "bruinsma\t3\t2.0\n",
      "jere\t2\t1.0\n",
      "euromobiliare\t5\t6.0\n",
      "largeness\t2\t3.0\n",
      "coenzyme\t3\t2.0\n",
      "hadler\t3\t2.0\n",
      "datapower\t3\t4.0\n",
      "whomsoever\t4\t3.0\n",
      "up\t1\t2.0\n",
      "reihl\t1\t2.0\n",
      "idealists\t3\t4.0\n",
      "wahine\t3\t2.0\n",
      "sporophyte\t2\t3.0\n",
      "feb\t4\t1.0\n",
      "koegler\t3\t2.0\n",
      "akao\t3\t2.0\n",
      "alves\t2\t1.0\n",
      "dsouza\t3\t2.0\n",
      "fluor\t2\t1.0\n",
      "cordial\t2\t3.0\n",
      "souring\t3\t2.0\n",
      "asia's\t2\t3.0\n",
      "joers\t2\t1.0\n",
      "gametangia\t5\t4.0\n",
      "reshuffling\t4\t3.0\n",
      "zentralsparkasse\t5\t4.0\n",
      "aluminosilicate\t7\t6.0\n",
      "amelia\t3\t4.0\n",
      "kaohsiung\t4\t3.0\n",
      "myojin\t2\t3.0\n",
      "asia\t2\t3.0\n",
      "evidentiary\t6\t5.0\n",
      "garcia\t3\t2.0\n",
      "livesay\t3\t2.0\n",
      "doable\t3\t2.0\n",
      "nordine\t3\t2.0\n",
      "rovaniemi\t5\t4.0\n",
      "lavere\t3\t2.0\n",
      "haydn\t2\t1.0\n",
      "goering\t3\t2.0\n",
      "cevaxs\t3\t2.0\n",
      "rahe\t2\t1.0\n",
      "allender\t4\t3.0\n",
      "dauenhauer\t4\t3.0\n",
      "pc's\t2\t3.0\n",
      "crippling\t3\t2.0\n",
      "reay\t1\t2.0\n",
      "heroized\t3\t2.0\n",
      "gruet\t2\t1.0\n",
      "hours\t2\t1.0\n",
      "eduardo\t3\t4.0\n",
      "jefferies's\t3\t4.0\n",
      "skrzypek\t3\t2.0\n",
      "beachler\t3\t2.0\n",
      "treml\t2\t1.0\n",
      "makegood\t2\t3.0\n",
      "hejl\t1\t2.0\n",
      "diabetes\t4\t3.0\n",
      "california's\t4\t5.0\n",
      "cianci\t2\t3.0\n",
      "rhodanthe\t2\t3.0\n",
      "privileged\t2\t3.0\n",
      "preciado\t3\t4.0\n",
      "reitano\t4\t3.0\n",
      "delatorre\t4\t3.0\n",
      "eap\t3\t1.0\n",
      "linearly\t4\t3.0\n",
      "sires\t1\t2.0\n",
      "ksiazek\t2\t3.0\n",
      "firecrackers\t4\t3.0\n",
      "agne\t2\t1.0\n",
      "giaimo\t2\t3.0\n",
      "morales\t2\t3.0\n",
      "gunslinger\t3\t4.0\n",
      "torres\t2\t1.0\n",
      "vliet\t1\t2.0\n",
      "mericantante\t5\t4.0\n",
      "gentlest\t3\t2.0\n",
      "brilliance\t2\t3.0\n",
      "siese\t2\t1.0\n",
      "stjohn\t2\t1.0\n",
      "tigges\t1\t2.0\n",
      "gateley\t3\t2.0\n",
      "appreciatively\t6\t5.0\n",
      "motorcyclists\t5\t4.0\n",
      "ciullo\t2\t3.0\n",
      "volentine\t4\t3.0\n",
      "nonbusiness\t3\t4.0\n",
      "petralia\t4\t3.0\n",
      "digioia\t3\t4.0\n",
      "koala\t3\t2.0\n",
      "manzanares\t4\t3.0\n",
      "przybysz\t3\t2.0\n",
      "tinkling\t3\t2.0\n",
      "vsel\t2\t1.0\n",
      "kalthoff\t1\t2.0\n",
      "mcalinden\t3\t4.0\n",
      "hirai\t3\t2.0\n",
      "reeducation\t5\t4.0\n",
      "hostettler\t4\t3.0\n",
      "soloviev\t3\t4.0\n",
      "heterogeneous\t5\t6.0\n",
      "tracheal\t3\t2.0\n",
      "dinmukhamed\t4\t3.0\n",
      "rieth\t2\t1.0\n",
      "cafes\t2\t1.0\n",
      "eranthe\t2\t3.0\n",
      "w's\t3\t1.0\n",
      "yzaguirre\t4\t3.0\n",
      "temperature\t3\t4.0\n",
      "guidone\t3\t2.0\n",
      "cruikshank\t3\t2.0\n",
      "vuong\t2\t1.0\n",
      "boaz\t2\t1.0\n",
      "safeco\t2\t3.0\n",
      "mcgeean's\t3\t2.0\n",
      "krajina's\t2\t3.0\n",
      "kanouse\t4\t2.0\n",
      "bukkake\t3\t2.0\n",
      "interviewee\t4\t3.0\n",
      "berghuis\t3\t2.0\n",
      "isosceles\t4\t3.0\n",
      "foresters\t3\t2.0\n",
      "emanuel\t4\t3.0\n",
      "sakau\t3\t2.0\n",
      "ameche\t3\t2.0\n",
      "entebbe\t3\t2.0\n",
      "villafane\t4\t3.0\n",
      "melone\t3\t2.0\n",
      "cardoen\t3\t2.0\n",
      "lawyer\t2\t3.0\n",
      "dlouhy\t3\t2.0\n",
      "theoreticians\t5\t4.0\n",
      "yamaichi's\t4\t3.0\n",
      "mobiliare\t3\t4.0\n",
      "marcial\t3\t2.0\n",
      "esquire's\t2\t3.0\n",
      "crocodilian\t4\t5.0\n",
      "mccaughan\t2\t3.0\n",
      "isoelectronic\t6\t5.0\n",
      "wyden\t1\t2.0\n",
      "laware\t2\t3.0\n",
      "epitome\t4\t3.0\n",
      "belittling\t4\t3.0\n",
      "dickensian\t4\t3.0\n",
      "irelands\t2\t3.0\n",
      "aisa\t3\t2.0\n",
      "heyen\t1\t2.0\n",
      "entreaties\t3\t4.0\n",
      "aviles\t3\t2.0\n",
      "kerestes\t2\t3.0\n",
      "rockne\t2\t1.0\n",
      "persians\t2\t3.0\n",
      "ultraviolet\t5\t4.0\n",
      "cruel\t2\t1.0\n",
      "compusa\t4\t3.0\n",
      "vires\t1\t2.0\n",
      "bartolomei\t5\t4.0\n",
      "fontainebleau\t3\t4.0\n",
      "iovine\t4\t3.0\n",
      "giovenco\t3\t4.0\n",
      "lbs\t1\t3.0\n",
      "monteverde\t4\t3.0\n",
      "wrangler\t3\t2.0\n",
      "truancy\t3\t2.0\n",
      "mayweather\t3\t4.0\n",
      "koernke\t3\t1.0\n",
      "auerback\t2\t3.0\n",
      "burciaga\t4\t3.0\n",
      "linehan\t3\t2.0\n",
      "judaism's\t4\t3.0\n",
      "denuclearized\t5\t4.0\n",
      "guacamole\t4\t3.0\n",
      "eleanore\t3\t4.0\n",
      "daoud\t1\t2.0\n",
      "hustling\t3\t2.0\n",
      "nkohse\t3\t1.0\n",
      "edizione\t5\t4.0\n",
      "hyogo\t2\t3.0\n",
      "abasia\t3\t4.0\n",
      "halcyon\t3\t2.0\n",
      "marketamerica\t6\t5.0\n",
      "alienate\t3\t4.0\n",
      "pharaon\t2\t3.0\n",
      "inconspicuous\t4\t5.0\n",
      "rna\t3\t2.0\n",
      "stapler\t3\t2.0\n",
      "przybyl\t3\t2.0\n",
      "interests\t2\t3.0\n",
      "condren\t3\t2.0\n",
      "vecchione\t4\t3.0\n",
      "polecat\t2\t3.0\n",
      "sightseers\t3\t2.0\n",
      "hour\t2\t1.0\n",
      "sichuan\t2\t3.0\n",
      "izquierdo\t3\t4.0\n",
      "lesieur\t3\t2.0\n",
      "sibling\t2\t3.0\n",
      "sterle\t2\t1.0\n",
      "mired\t1\t2.0\n",
      "aires\t2\t1.0\n",
      "deactivate\t4\t3.0\n",
      "ngo\t2\t3.0\n",
      "kindler\t3\t2.0\n",
      "prayerful\t2\t3.0\n",
      "dreher\t1\t2.0\n",
      "stangeland\t3\t2.0\n",
      "semireligious\t5\t6.0\n",
      "leant\t2\t1.0\n",
      "san-jose\t3\t2.0\n",
      "trivialized\t4\t3.0\n",
      "spacesuit\t2\t3.0\n",
      "poinsettias\t4\t3.0\n",
      "lineberger\t3\t4.0\n",
      "reeves's\t2\t3.0\n",
      "malave\t3\t2.0\n",
      "loiseau\t3\t2.0\n",
      "magnolias\t3\t4.0\n",
      "sangiovese\t4\t3.0\n",
      "mcdyess\t3\t2.0\n",
      "settling\t3\t2.0\n",
      "schoeffler\t3\t2.0\n",
      "riordan\t2\t3.0\n",
      "lesbianism\t4\t5.0\n",
      "coordinated\t5\t4.0\n",
      "delores\t3\t2.0\n",
      "uncoordinated\t6\t5.0\n",
      "schettler\t3\t2.0\n",
      "phenolphthalein\t4\t5.0\n",
      "prentnieks\t3\t2.0\n",
      "ritualistically\t7\t6.0\n",
      "giampaolo\t3\t4.0\n",
      "dettore\t3\t2.0\n",
      "janii\t2\t3.0\n",
      "caradine\t4\t3.0\n",
      "grueling\t3\t2.0\n",
      "jocylan\t2\t3.0\n",
      "louisiana's\t5\t4.0\n",
      "denarii\t3\t4.0\n",
      "waymire\t2\t3.0\n",
      "derouen\t2\t3.0\n",
      "elizalde\t4\t3.0\n",
      "cspi\t4\t2.0\n",
      "dr.\t1\t3.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(error_list)):\n",
    "    print(error_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM_NJYjpJLAb"
   },
   "source": [
    "**Note:** it can be seen the errors are acceptable. The differences between predictions and real labels are often `1.0` syllable. I believe this would not affect the final result of the reading level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iBWmK6GHwCt"
   },
   "source": [
    "## ***6. Deploy model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1Uhl_CCKdkz"
   },
   "source": [
    "### ***6.1 On words***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "bKDIRVJROX4X"
   },
   "outputs": [],
   "source": [
    "def tokenize_word(words):\n",
    "\n",
    "  \n",
    "    test_ints = []\n",
    "    for word in words:\n",
    "      word = word.lower()\n",
    "      test_int = [char_to_int[char] for char in word if char in char_to_int]\n",
    "      if len(test_int) > 0:\n",
    "          test_ints.append(test_int)\n",
    "    test_ints = pad_words(test_ints, 28)\n",
    "    return test_ints\n",
    "\n",
    "def predict(net, words):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "\n",
    "    test_ints = tokenize_word(words)\n",
    "    print(test_ints)\n",
    "    \n",
    "\n",
    "    features = np.array(test_ints)\n",
    "    \n",
    "\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    batch_size =  feature_tensor.size(0)\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    pred = torch.round(output.squeeze()/1000)\n",
    "    print(pred.data) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: ', (output.data))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qK8j6D_BOcnt",
    "outputId": "56e6e51c-3dd0-4173-e5e4-6086696478ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  14  9 19 21]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  13  6 12 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  9  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 10 16  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 24\n",
      "  16  4 11  3]]\n",
      "tensor([2., 1., 1., 1., 1.], device='cuda:0')\n",
      "Prediction value, pre-rounding:  tensor([2042.9443, 1031.2249, 1153.6891, 1039.3712, 1013.9448],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predict(net_lstm, ['moby', 'dick', 'or', 'the', 'whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01bAa_TlKXtD"
   },
   "source": [
    "### ***6.2 On text file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "C1hfMmj5VxXk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Project_SQT unidirectional LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
